{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b345ad6a-0fe7-4588-b9f7-581422ea8314",
   "metadata": {},
   "source": [
    "# Download modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6513b6aa-8d1a-4b71-add4-2f6d58d3cdf8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T17:40:14.500335Z",
     "iopub.status.busy": "2022-11-28T17:40:14.499810Z",
     "iopub.status.idle": "2022-11-28T17:40:24.610821Z",
     "shell.execute_reply": "2022-11-28T17:40:24.609325Z",
     "shell.execute_reply.started": "2022-11-28T17:40:14.500305Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting segmentation_models_pytorch\n",
      "  Downloading segmentation_models_pytorch-0.3.0-py3-none-any.whl (97 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting timm==0.4.12\n",
      "  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pretrainedmodels==0.7.4\n",
      "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from segmentation_models_pytorch) (4.64.0)\n",
      "Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.9/dist-packages (from segmentation_models_pytorch) (0.13.0+cu116)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.9/dist-packages (from segmentation_models_pytorch) (9.2.0)\n",
      "Collecting efficientnet-pytorch==0.7.1\n",
      "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.12.0+cu116)\n",
      "Collecting munch\n",
      "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (4.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (2.28.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (1.23.1)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from munch->pretrainedmodels==0.7.4->segmentation_models_pytorch) (1.14.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (1.26.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (2.8)\n",
      "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n",
      "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16428 sha256=099dea530018f871e52454ba7303cdc5182262c37f6fc9783436ae5671e0a651\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/16/24/752e89d88d333af39a288421e64d613b5f652918e39ef1f8e3\n",
      "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60943 sha256=32247bf91797e8722e5c012776eb670ee525e83434a8e74c80e8833b6fd7e280\n",
      "  Stored in directory: /root/.cache/pip/wheels/d1/3b/4e/2f3015f1ab76f34be28e04c4bcee27e8cabfa70d2eadf8bc3b\n",
      "Successfully built efficientnet-pytorch pretrainedmodels\n",
      "Installing collected packages: munch, efficientnet-pytorch, timm, pretrainedmodels, segmentation_models_pytorch\n",
      "Successfully installed efficientnet-pytorch-0.7.1 munch-2.5.0 pretrainedmodels-0.7.4 segmentation_models_pytorch-0.3.0 timm-0.4.12\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install segmentation_models_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d0136c6-fba8-461c-91cc-805cd5d9406e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T17:47:46.082909Z",
     "iopub.status.busy": "2022-11-28T17:47:46.082439Z",
     "iopub.status.idle": "2022-11-28T17:47:52.971396Z",
     "shell.execute_reply": "2022-11-28T17:47:52.970227Z",
     "shell.execute_reply.started": "2022-11-28T17:47:46.082882Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting SimpleITK\n",
      "  Downloading SimpleITK-2.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: SimpleITK\n",
      "Successfully installed SimpleITK-2.2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install SimpleITK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5664e27f-aff6-4804-89b7-242d22b99b59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T17:48:09.043408Z",
     "iopub.status.busy": "2022-11-28T17:48:09.042914Z",
     "iopub.status.idle": "2022-11-28T17:48:14.062836Z",
     "shell.execute_reply": "2022-11-28T17:48:14.061332Z",
     "shell.execute_reply.started": "2022-11-28T17:48:09.043382Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nibabel\n",
      "  Downloading nibabel-4.0.2-py3-none-any.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nibabel) (63.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from nibabel) (1.23.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.9/dist-packages (from nibabel) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging>=17.0->nibabel) (3.0.9)\n",
      "Installing collected packages: nibabel\n",
      "Successfully installed nibabel-4.0.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install nibabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a4b545e-ad3b-4268-b15c-145636ad7def",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T17:48:20.704663Z",
     "iopub.status.busy": "2022-11-28T17:48:20.704260Z",
     "iopub.status.idle": "2022-11-28T17:48:21.689568Z",
     "shell.execute_reply": "2022-11-28T17:48:21.688328Z",
     "shell.execute_reply.started": "2022-11-28T17:48:20.704634Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import time\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "import torch\n",
    "import os\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import os\n",
    "import SimpleITK as sitk\n",
    "import nibabel as nib\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1dabe570-311e-47f1-b110-d230d325a00e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T18:14:48.926099Z",
     "iopub.status.busy": "2022-11-28T18:14:48.925554Z",
     "iopub.status.idle": "2022-11-28T18:14:48.949105Z",
     "shell.execute_reply": "2022-11-28T18:14:48.947721Z",
     "shell.execute_reply.started": "2022-11-28T18:14:48.926099Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import SimpleITK as sitk\n",
    "import nibabel as nib\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "# Function to load yaml configuration file\n",
    "def load_config(config_name):\n",
    "    \"\"\"\n",
    "    Load configuration file\n",
    "    :param config_name:\n",
    "    :return: configuration data\n",
    "    \"\"\"\n",
    "    with open(config_name) as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "def load_mask(file):\n",
    "    \"\"\"\n",
    "    Load masks from .gz files\n",
    "\n",
    "    :param file: input filename\n",
    "    :return: array of 2D masks (z, x, y)\n",
    "    \"\"\"\n",
    "    mask = nib.load(file)\n",
    "    mask = mask.get_fdata().transpose(2, 0, 1)\n",
    "    mask = np.rot90(mask, axes=(1, 2))\n",
    "    mask = mask.astype('int')\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def load_dicom(directory):\n",
    "    \"\"\"\n",
    "    Load images from .dcm files\n",
    "\n",
    "    :param directory: directory with .dcm files\n",
    "    :return: array of 2D images (z, x, y)\n",
    "    \"\"\"\n",
    "    reader = sitk.ImageSeriesReader()\n",
    "    dicom_names = reader.GetGDCMSeriesFileNames(directory)\n",
    "    reader.SetFileNames(dicom_names)\n",
    "    image_itk = reader.Execute()\n",
    "    image_zyx = sitk.GetArrayFromImage(image_itk).astype(np.int16)\n",
    "    image_zyx = image_zyx.astype('int')\n",
    "\n",
    "    return image_zyx\n",
    "\n",
    "\n",
    "def preprocess_dataset(dir_original_images,\n",
    "                       dir_original_masks,\n",
    "                       dir_processed, save=True, output=False):\n",
    "    \"\"\"\n",
    "    Load original images and masks from input directories\n",
    "    and store them as numpy arrays\n",
    "\n",
    "    :param dir_original_images: directory with input images\n",
    "    :param dir_original_masks: directory with input masks\n",
    "    :param dir_processed: directory for processed data\n",
    "    :param save: save processed images and masks to .npz files and file with meta-data\n",
    "    :param output: output processed images and masks\n",
    "\n",
    "    :return array of images (z, x, y) and masks (z, x, y) if param output=True\n",
    "    \"\"\"\n",
    "\n",
    "    images = np.empty(shape=(0, 512, 512)).astype('int')\n",
    "    masks = np.empty(shape=(0, 512, 512)).astype('int')\n",
    "\n",
    "    images_folders = [str(name) for name in\n",
    "                      list(Path(os.path.join(dir_original_images)).rglob(\"**\"))\n",
    "                      if os.listdir(name)[0].endswith('.dcm')]\n",
    "\n",
    "    masks_values = []\n",
    "\n",
    "    for folder in images_folders:\n",
    "\n",
    "        images = np.concatenate((images, load_dicom(folder)), axis=0)\n",
    "\n",
    "        patient = re.findall(r'LUNG[0-9]*-[0-9]*', folder)[0]\n",
    "\n",
    "        mask_file = str(list(Path(os.path.join(dir_original_masks, patient))\n",
    "                             .rglob(\"*.gz\"))[0])\n",
    "\n",
    "        masks_batch = load_mask(mask_file)\n",
    "\n",
    "        if save:\n",
    "            masks_values = masks_values + [[patient, 1] if mask.sum() > 0 else [patient, 0]\n",
    "                                           for mask in masks_batch]\n",
    "\n",
    "        masks = np.concatenate((masks, masks_batch), axis=0)\n",
    "\n",
    "    print(f'Images read: {images.shape[0]}')\n",
    "    print(f'Masks read: {masks.shape[0]}')\n",
    "\n",
    "    if save:\n",
    "        np.savez_compressed(os.path.join(dir_processed, 'images'), *[image for image in images])\n",
    "        np.savez_compressed(os.path.join(dir_processed, 'masks'), *[mask for mask in masks])\n",
    "\n",
    "        masks_df = pd.DataFrame(data=masks_values, columns=['Patient', 'Effusion'])\n",
    "\n",
    "        masks_df.to_csv(os.path.join(dir_processed, 'meta_file.csv'),\n",
    "                        sep='\\t', index_label='index')\n",
    "\n",
    "    if output:\n",
    "        return images, masks\n",
    "\n",
    "\n",
    "def show_dice_example(mask1, mask2, dice_coef):\n",
    "    \"\"\"\n",
    "    Show plots with two binary masks and dice coefficient between them\n",
    "    :param mask1:  binary mask [n, z, x, y]\n",
    "    :param mask2: binary mask [n, z,  x, y]\n",
    "    :param dice_coef: dice coefficient\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    plt.suptitle(f'DICE coefficient: {dice_coef:.2f}')\n",
    "\n",
    "    ax[0].imshow(mask1[0].permute(1, 2, 0))\n",
    "    ax[1].imshow(mask2[0].permute(1, 2, 0))\n",
    "\n",
    "    ax[0].set_title('Mask 1')\n",
    "    ax[1].set_title('Mask 2')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def save_history(history_dict, output_folder):\n",
    "    \"\"\"\n",
    "    Save training history file to csv\n",
    "    :param history_dict:  dictionary with training history\n",
    "    :param output_folder:  folder to write csv\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pd.DataFrame(history_dict).to_csv(os.path.join(output_folder, 'train_history.csv'),\n",
    "                                          index_label='epoch', sep='\\t', float_format='%.3f')\n",
    "    except IOError:\n",
    "        print(\"I/O error\")\n",
    "\n",
    "\n",
    "\n",
    "def  save_dice_image(epoch, dice, output_dir):\n",
    "    \"\"\"\n",
    "    # Save plot of dice coefficient over epoch\n",
    "    :param epoch:  current epoch\n",
    "    :param dice:  dice coefficient value\n",
    "    :param: output_dir: output folder\n",
    "    \"\"\"\n",
    "\n",
    "    epochs = [i+1 for i in range(epoch+1)]\n",
    "    plt.plot(epochs, dice)\n",
    "    plt.title('DICE coefficient for validation set')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('DICE coef')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xlim(1, epoch+1)\n",
    "    plt.xticks(epochs)\n",
    "    plt.grid(visible=True)\n",
    "    plt.savefig(os.path.join(output_dir, 'DICE_coef.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50cd839f-49dc-4b8e-999a-a1f5ed9b5709",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T17:43:46.691325Z",
     "iopub.status.busy": "2022-11-28T17:43:46.690493Z",
     "iopub.status.idle": "2022-11-28T17:43:48.805470Z",
     "shell.execute_reply": "2022-11-28T17:43:48.804567Z",
     "shell.execute_reply.started": "2022-11-28T17:43:46.691296Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms as T\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, dir_input, test_size, is_valid=None, normalization=None):\n",
    "        \"\"\"\n",
    "        Initialize images and masks datasets\n",
    "        :param dir_input: directory with input data\n",
    "        :param test_size: size of validation dataset\n",
    "        :param is_valid: defines validation or training set\n",
    "        :param normalization: normalization to be applied to input images\n",
    "        \"\"\"\n",
    "\n",
    "        # Read images\n",
    "        self.images = np.load(os.path.join(dir_input, 'images.npz'))\n",
    "\n",
    "        # Read masks\n",
    "        self.masks = np.load(os.path.join(dir_input, 'masks.npz'))\n",
    "\n",
    "        # Read meta-file\n",
    "        self.meta = pd.read_csv(os.path.join(dir_input,\n",
    "                                             'meta_file.csv'), sep='\\t')\n",
    "\n",
    "        # Split data into train and validation sets if necessary\n",
    "        if is_valid:\n",
    "            idx_train, idx_val, _, _ = train_test_split(self.meta['index'].values,\n",
    "                                                        self.meta['Effusion'].values,\n",
    "                                                        test_size=test_size,\n",
    "                                                        random_state=42)\n",
    "            if is_valid == True:\n",
    "                self.meta = self.meta.iloc[idx_val]\n",
    "            else:\n",
    "                self.meta = self.meta.iloc[idx_train]\n",
    "\n",
    "        if normalization:\n",
    "            self.normalization = T.Normalize(std=443, mean=-720)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get item from dataset according to index from meta-file\n",
    "        :param idx: index\n",
    "        \"\"\"\n",
    "        index = 'arr_' + str(self.meta.iloc[idx]['index'])\n",
    "\n",
    "        mask = T.ToTensor()(self.masks[index]).type(torch.float)\n",
    "        image = T.ToTensor()(self.images[index]).type(torch.float)\n",
    "\n",
    "        if self.normalization:\n",
    "            image = self.normalization(image)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Get length of dataset\n",
    "        \"\"\"\n",
    "        return len(self.meta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e94e55e4-9329-40f8-8dd8-24b7ab52ca27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T17:43:48.807794Z",
     "iopub.status.busy": "2022-11-28T17:43:48.807279Z",
     "iopub.status.idle": "2022-11-28T17:43:48.825614Z",
     "shell.execute_reply": "2022-11-28T17:43:48.824436Z",
     "shell.execute_reply.started": "2022-11-28T17:43:48.807775Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Conv2d, ReLU, ConvTranspose2d, MaxPool2d, Module, ModuleList\n",
    "from torchvision.transforms import CenterCrop\n",
    "\n",
    "\n",
    "class Block(Module):\n",
    "    def __init__(self, inChannels, outChannels):\n",
    "        super().__init__()\n",
    "\n",
    "        # Convolution and RELU layers\n",
    "        self.conv1 = Conv2d(inChannels, outChannels, 3)\n",
    "        self.relu = ReLU()\n",
    "        self.conv2 = Conv2d(outChannels, outChannels, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply CONV => RELU => CONV block to the inputs and return it\n",
    "        return self.conv2(self.relu(self.conv1(x)))\n",
    "\n",
    "\n",
    "class Encoder(Module):\n",
    "    def __init__(self, channels=(3, 16, 32, 64)):\n",
    "        super().__init__()\n",
    "\n",
    "        # Eencoder blocks and maxpooling layer\n",
    "        self.encBlocks = ModuleList(\n",
    "            [Block(channels[i], channels[i + 1])\n",
    "             for i in range(len(channels) - 1)])\n",
    "\n",
    "        self.pool = MaxPool2d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # initialize an empty list to store the intermediate outputs\n",
    "        blockOutputs = []\n",
    "\n",
    "        # loop through the encoder blocks\n",
    "        for block in self.encBlocks:\n",
    "            # pass the inputs through the current encoder block, store\n",
    "            # the outputs, and then apply maxpooling on the output\n",
    "            x = block(x)\n",
    "            blockOutputs.append(x)\n",
    "            x = self.pool(x)\n",
    "        # return the list containing the intermediate outputs\n",
    "        return blockOutputs\n",
    "\n",
    "\n",
    "class Decoder(Module):\n",
    "    def __init__(self, channels=(64, 32, 16)):\n",
    "        super().__init__()\n",
    "        # initialize the number of channels, upsampler blocks, and\n",
    "        # decoder blocks\n",
    "        self.channels = channels\n",
    "        self.upconvs = ModuleList(\n",
    "            [ConvTranspose2d(channels[i], channels[i + 1], 2, 2)\n",
    "             for i in range(len(channels) - 1)])\n",
    "        self.dec_blocks = ModuleList(\n",
    "            [Block(channels[i], channels[i + 1])\n",
    "             for i in range(len(channels) - 1)])\n",
    "\n",
    "    def forward(self, x, encFeatures):\n",
    "        # loop through the number of channels\n",
    "        for i in range(len(self.channels) - 1):\n",
    "            # pass the inputs through the upsampler blocks\n",
    "            x = self.upconvs[i](x)\n",
    "            # crop the current features from the encoder blocks,\n",
    "            # concatenate them with the current upsampled features,\n",
    "            # and pass the concatenated output through the current\n",
    "            # decoder block\n",
    "            encFeat = self.crop(encFeatures[i], x)\n",
    "            x = torch.cat([x, encFeat], dim=1)\n",
    "            x = self.dec_blocks[i](x)\n",
    "        # return the final decoder output\n",
    "        return x\n",
    "\n",
    "    def crop(self, encFeatures, x):\n",
    "        # grab the dimensions of the inputs, and crop the encoder\n",
    "        # features to match the dimensions\n",
    "        (_, _, H, W) = x.shape\n",
    "        encFeatures = CenterCrop([H, W])(encFeatures)\n",
    "        # return the cropped features\n",
    "        return encFeatures\n",
    "\n",
    "\n",
    "class UNet(Module):\n",
    "    def __init__(self, outSize, encChannels=(1, 16, 32, 64),\n",
    "                 decChannels=(64, 32, 16),\n",
    "                 nbClasses=1, retainDim=True):\n",
    "        super().__init__()\n",
    "        # initialize the encoder and decoder\n",
    "        self.encoder = Encoder(encChannels)\n",
    "        self.decoder = Decoder(decChannels)\n",
    "\n",
    "        # initialize the regression head and store the class variables\n",
    "        self.head = Conv2d(decChannels[-1], nbClasses, 1)\n",
    "        self.retainDim = retainDim\n",
    "        self.outSize = outSize\n",
    "\n",
    "    def forward(self, x):\n",
    "        # grab the features from the encoder\n",
    "        encFeatures = self.encoder(x)\n",
    "\n",
    "        # pass the encoder features through decoder making sure that\n",
    "        # their dimensions are suited for concatenation\n",
    "        decFeatures = self.decoder(encFeatures[::-1][0],\n",
    "                                   encFeatures[::-1][1:])\n",
    "\n",
    "        # pass the decoder features through the regression head to\n",
    "        # obtain the segmentation mask\n",
    "        map = self.head(decFeatures)\n",
    "\n",
    "        # check to see if we are retaining the original output\n",
    "        # dimensions and if so, then resize the output to match them\n",
    "        if self.retainDim:\n",
    "            map = F.interpolate(map, self.outSize)\n",
    "\n",
    "        # return the segmentation map\n",
    "        return map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d534471-c3d4-4b24-91ce-8462fb2b08c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T17:43:48.826715Z",
     "iopub.status.busy": "2022-11-28T17:43:48.826251Z",
     "iopub.status.idle": "2022-11-28T17:43:48.835666Z",
     "shell.execute_reply": "2022-11-28T17:43:48.834320Z",
     "shell.execute_reply.started": "2022-11-28T17:43:48.826715Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "def dice_coefficient(mask1, mask2):\n",
    "    \"\"\"\n",
    "    Calculate DICE coefficient between masks in batch\n",
    "    :param mask1: predicted mask\n",
    "    :param mask2: ground truth mask\n",
    "    :return: DICE coefficient\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate  summary area for each masks\n",
    "    sum1 = mask1.sum(axis=(1, 2, 3))\n",
    "    sum2 = mask2.sum(axis=(1, 2, 3))\n",
    "\n",
    "    # Calculate intersection\n",
    "    intersection = (mask1 * mask2).sum(axis=(1, 2, 3))\n",
    "\n",
    "    #  Calculate mean dice coefficient (set value to 1 if both masks are empty)\n",
    "    coefficient = (2 * intersection / (sum1 + sum2)).nan_to_num(1.).mean().item()\n",
    "\n",
    "    return  coefficient\n",
    "\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        # comment out if your model contains a sigmoid or equivalent activation layer\n",
    "\n",
    "        # flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice = (2. * intersection + smooth) / (inputs.sum() + targets.sum() + smooth)\n",
    "\n",
    "        return 1 - dice\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a5882e82-4a37-494d-b33a-ee30d8ea6586",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T18:19:12.314207Z",
     "iopub.status.busy": "2022-11-28T18:19:12.313825Z",
     "iopub.status.idle": "2022-11-28T18:19:12.320840Z",
     "shell.execute_reply": "2022-11-28T18:19:12.319924Z",
     "shell.execute_reply.started": "2022-11-28T18:19:12.314169Z"
    }
   },
   "outputs": [],
   "source": [
    "config = {'dataset': \n",
    "          {'dir_images': 'data/raw/images/', \n",
    "           'dir_masks': 'datsets/data/raw/masks/', \n",
    "           'dir_processed': '/datasets/pleural-effusion/', \n",
    "           'image_height': 512, \n",
    "           'image_width': 512}, \n",
    "          \n",
    "          'model': {'architecture': 'UNet', \n",
    "                    'dir_output': 'ouput/'}, \n",
    "          \n",
    "          'train': {'test_size': 0.2, \n",
    "                    'batch_size': 8, \n",
    "                    'device': 'cuda', \n",
    "                    'learning_rate': 2e-05, \n",
    "                    'beta1': 0.9, 'beta2': 0.999, \n",
    "                    'num_epochs': 3, \n",
    "                    'decay_rate': 1}, \n",
    "          \n",
    "          'evaluate': {'batch_size': 8, \n",
    "                       'device': 'cuda', \n",
    "                       'threshold': 0.5}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa3c6ed7-ada0-4fc3-a55f-ebc8083f7b96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T17:52:50.682759Z",
     "iopub.status.busy": "2022-11-28T17:52:50.682380Z",
     "iopub.status.idle": "2022-11-28T17:52:50.842099Z",
     "shell.execute_reply": "2022-11-28T17:52:50.840916Z",
     "shell.execute_reply.started": "2022-11-28T17:52:50.682733Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create train dataset\n",
    "train_dataset = ImageDataset(dir_input=config['dataset']['dir_processed'],\n",
    "                             test_size=config['train']['test_size'],\n",
    "                             is_valid=False,\n",
    "                             normalization=True)\n",
    "\n",
    "# Create validation dataset\n",
    "valid_dataset = ImageDataset(dir_input=config['dataset']['dir_processed'],\n",
    "                             test_size=config['train']['test_size'],\n",
    "                             is_valid=True,\n",
    "                             normalization=True)\n",
    "\n",
    "# Create dataloader for train dataset\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              batch_size=config['train']['batch_size'],\n",
    "                              shuffle=True)\n",
    "\n",
    "# Create dataloader for validation dataset\n",
    "valid_dataloader = DataLoader(valid_dataset,\n",
    "                              batch_size=config['evaluate']['batch_size'],\n",
    "                              shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "59d01a28-fe98-4f6b-9932-71466352b3ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T18:16:43.785278Z",
     "iopub.status.busy": "2022-11-28T18:16:43.784841Z",
     "iopub.status.idle": "2022-11-28T18:16:45.029513Z",
     "shell.execute_reply": "2022-11-28T18:16:45.027295Z",
     "shell.execute_reply.started": "2022-11-28T18:16:43.785250Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training begin...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/notebooks/ouput/model_state_best.ckpt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [40]\u001b[0m, in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m avg_valid_loss \u001b[38;5;241m<\u001b[39m best_loss_score:\n\u001b[1;32m    112\u001b[0m     best_loss_score \u001b[38;5;241m=\u001b[39m avg_valid_loss\n\u001b[0;32m--> 113\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m               \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdir_output\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_state_best.ckpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Update training results\u001b[39;00m\n\u001b[1;32m    118\u001b[0m history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(avg_train_loss)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/serialization.py:376\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;124;03m\"\"\"save(obj, f, pickle_module=pickle, pickle_protocol=DEFAULT_PROTOCOL, _use_new_zipfile_serialization=True)\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \n\u001b[1;32m    342\u001b[0m \u001b[38;5;124;03mSaves an object to a disk file.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;124;03m    >>> torch.save(x, buffer)\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    374\u001b[0m _check_dill_version(pickle_module)\n\u001b[0;32m--> 376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    378\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(opened_file) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 230\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/notebooks/ouput/model_state_best.ckpt'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define model\n",
    "net = UNet(outSize=(config['dataset']['image_height'],\n",
    "                        config['dataset']['image_width'])).to(config['train']['device'])\n",
    "\n",
    "# net.load_state_dict(torch.load('output/model_state_final_6.ckpt'))\n",
    "\n",
    "# Define loss function\n",
    "criterion  =   smp.losses.DiceLoss(mode='binary')\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = Adam(net.parameters(),\n",
    "                 lr=config['train']['learning_rate'],\n",
    "                 betas=(config['train']['beta1'], config['train']['beta2']))\n",
    "\n",
    "# Define learning rate scheduler\n",
    "lambda_decay = lambda epoch: config['train']['decay_rate'] ** epoch\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_decay)\n",
    "\n",
    "print(\"[INFO] Training begin...\")\n",
    "\n",
    "# Collect training results\n",
    "history = {}\n",
    "history['train_loss'] = []\n",
    "history['train_dice'] = []\n",
    "history['valid_loss'] = []\n",
    "history['valid_dice'] = []\n",
    "\n",
    "best_loss_score = np.Inf\n",
    "epochs = config['train']['num_epochs']\n",
    "start_time = time()\n",
    "\n",
    "# Iterate through epoches\n",
    "for epoch in range(epochs):\n",
    "    # Set network to train mode\n",
    "    net.train()\n",
    "\n",
    "    # Initialize train and validation losses and dice values for an epoch\n",
    "    train_loss_total = 0\n",
    "    valid_loss_total = 0\n",
    "    train_dice_total = 0\n",
    "    valid_dice_total = 0\n",
    "\n",
    "    # Check learning rate\n",
    "    learning_rate = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "    # Iterate through dataloader\n",
    "    for (image, mask) in train_dataloader:\n",
    "        # Send input to device\n",
    "        image = image.to(config['train']['device'])\n",
    "        mask = mask.to(config['train']['device'])\n",
    "\n",
    "        # Make prediction\n",
    "        pred = net(image)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(pred, mask)\n",
    "\n",
    "        # Zero previous gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Add loss\n",
    "        train_loss_total += loss.item()\n",
    "\n",
    "        # Add dice value\n",
    "        pred_mask = (torch.sigmoid(pred) > config['evaluate']['threshold'])\n",
    "        train_dice_total += dice_coefficient(pred_mask.detach().cpu(), mask.detach().cpu())\n",
    "        break\n",
    "\n",
    "    # Turn off autogradient\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Set model to evaluation mode\n",
    "        net.eval()\n",
    "\n",
    "        # Iterate through dataloader\n",
    "        for (image, mask) in valid_dataloader:\n",
    "            # Send input to device\n",
    "            image = image.to(config['train']['device'])\n",
    "            mask = mask.to(config['train']['device'])\n",
    "\n",
    "            # Make prediction\n",
    "            pred = net(image)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(pred, mask)\n",
    "\n",
    "            # Add loss\n",
    "            valid_loss_total += loss.item()\n",
    "\n",
    "            # Add dice value\n",
    "            pred_mask = (torch.sigmoid(pred) > config['evaluate']['threshold'])\n",
    "            valid_dice_total += dice_coefficient(pred_mask.detach().cpu(), mask.detach().cpu())\n",
    "            break\n",
    "\n",
    "    # Step learning rate scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    # Calculate average loss and dice per epoch\n",
    "    avg_train_loss = train_loss_total / (len(train_dataloader))\n",
    "    avg_valid_loss = valid_loss_total / (len(valid_dataloader))\n",
    "    avg_train_dice = train_dice_total / (len(train_dataloader))\n",
    "    avg_valid_dice = valid_dice_total / (len(valid_dataloader))\n",
    "\n",
    "    # Check if loss on current epoch is the best\n",
    "    if avg_valid_loss < best_loss_score:\n",
    "        best_loss_score = avg_valid_loss\n",
    "        torch.save(net.state_dict(),\n",
    "                   os.path.join(config['model']['dir_output'],\n",
    "                                'model_state_best.ckpt'))\n",
    "\n",
    "        \n",
    "    # Update training results\n",
    "    history['train_loss'].append(avg_train_loss)\n",
    "    history['valid_loss'].append(avg_valid_loss)\n",
    "    history['train_dice'].append(avg_train_dice)\n",
    "    history['valid_dice'].append(avg_valid_dice)\n",
    "\n",
    "    save_dice_image(epoch, history['valid_dice'], config['model']['dir_output'])\n",
    "    \n",
    "    \n",
    "    # Print epoch results\n",
    "    print('Epoch %3d/%3d, train loss: %5.3f, val loss: %5.3f, train dice: %5.3f, val dice: %5.3f, lr: %5.7f' % \\\n",
    "          (epoch + 1, epochs, avg_train_loss, avg_valid_loss, avg_train_dice, avg_valid_dice, learning_rate))\n",
    "\n",
    "# Save history file\n",
    "save_history(history, output_folder=config['model']['dir_output'])\n",
    "\n",
    "# Print time statistics\n",
    "end_time = time()\n",
    "print()\n",
    "print('Time total:     %5.2f sec' % (end_time - start_time))\n",
    "print('Time per epoch: %5.2f sec' % ((end_time - start_time) / epochs))\n",
    "print()\n",
    "\n",
    "torch.save(net.state_dict(),\n",
    "           os.path.join(config['model']['dir_output'],\n",
    "                        'model_state_final_v1.ckpt'))\n",
    "\n",
    "print(\"[INFO] Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "55c43fdc-efe5-42db-98d5-b0314f90e946",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T18:18:35.303076Z",
     "iopub.status.busy": "2022-11-28T18:18:35.302653Z",
     "iopub.status.idle": "2022-11-28T18:18:35.545467Z",
     "shell.execute_reply": "2022-11-28T18:18:35.544225Z",
     "shell.execute_reply.started": "2022-11-28T18:18:35.303048Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot([1,2,3])\n",
    "plt.savefig('output/temp.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e636680a-3b15-47ba-90cf-84afa389a84b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
